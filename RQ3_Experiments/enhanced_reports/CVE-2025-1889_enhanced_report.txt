CVE-2025-1889 exposes a critical flaw in versions of the picklescan tool prior
to 0.0.22, developed by mmaitre314, which fails to inspect embedded pickle files
unless they use standard file extensions. This oversight allows adversaries to
craft malicious PyTorch model archives that include non-standard pickle files
containing arbitrary code, effectively bypassing picklescan’s security checks.
The vulnerability stems from an overreliance on file naming conventions rather
than content inspection, leading to a false sense of safety when models are
scanned. Exploitation is feasible remotely, as attackers can distribute
compromised models via public repositories or internal package managers without
requiring physical access or elevated privileges. The attack leverages
torch.load() to deserialize the malicious payload, which is triggered when a
user loads the model—typically during inference or script execution—making user
interaction a necessary component of the exploit chain. Although the
vulnerability does not alter system boundaries, it enables code execution within
the host application’s context, potentially allowing attackers to exfiltrate
sensitive data, tamper with files, or degrade system performance depending on
the embedded payload. The technical complexity of the attack is moderate; while
it requires familiarity with PyTorch internals and model packaging, it does not
depend on unpredictable environmental factors and can be reliably reproduced.
Real-world scenarios include poisoned models introduced into machine learning
pipelines, where unsuspecting users deploy compromised assets into production,
leading to data breaches or operational disruption. Mitigation involves
upgrading to picklescan version 0.0.22 or later, which expands the scanning
scope to include non-standard extensions, thereby closing the detection gap.
Additionally, organizations should implement strict validation of third-party
models and consider sandboxing deserialization operations to limit potential
damage. Based on the available technical details and reproducibility of the
exploit, confidence in the assessment is high, and the vulnerability warrants
immediate attention due to its potential to undermine confidentiality,
integrity, and availability in environments that rely on automated model
ingestion.