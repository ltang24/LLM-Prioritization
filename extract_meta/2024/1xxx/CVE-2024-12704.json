{
  "CVE Code": "CVE-2024-12704",
  "Published": "2025-03-20T10:09:06.689Z",
  "Updated": "2025-03-20T18:58:07.026Z",
  "Description": "A vulnerability in the LangChainLLM class of the run-llama/llama_index repository, version v0.12.5, allows for a Denial of Service (DoS) attack. The stream_complete method executes the llm using a thread and retrieves the result via the get_response_gen method of the StreamingGeneratorCallbackHandler class. If the thread terminates abnormally before the _llm.predict is executed, there is no exception handling for this case, leading to an infinite loop in the get_response_gen function. This can be triggered by providing an input of an incorrect type, causing the thread to terminate and the process to continue running indefinitely.",
  "ProblemTypes": [
    "CWE-755 Improper Handling of Exceptional Conditions"
  ],
  "Affected": [
    {
      "vendor": "run-llama",
      "product": "run-llama/llama_index",
      "versions": [
        "unspecified"
      ]
    }
  ],
  "Reference": [
    "https://huntr.com/bounties/a0b638fd-21c6-4ba7-b381-6ab98472a02a",
    "https://github.com/run-llama/llama_index/commit/d1ecfb77578d089cbe66728f18f635c09aa32a05"
  ]
}