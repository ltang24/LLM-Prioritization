{
  "CVE Code": "CVE-2025-52566",
  "Published": "2025-06-24T03:21:19.009Z",
  "Updated": "2025-06-24T21:49:53.200Z",
  "Description": "llama.cpp is an inference of several LLM models in C/C++. Prior to version b5721, there is a signed vs. unsigned integer overflow in llama.cpp's tokenizer implementation (llama_vocab::tokenize) (src/llama-vocab.cpp:3036) resulting in unintended behavior in tokens copying size comparison. Allowing heap-overflowing llama.cpp inferencing engine with carefully manipulated text input during tokenization process. This issue has been patched in version b5721.",
  "ProblemTypes": [
    "CWE-119: Improper Restriction of Operations within the Bounds of a Memory Buffer",
    "CWE-195: Signed to Unsigned Conversion Error"
  ],
  "Affected": [
    {
      "vendor": "ggml-org",
      "product": "llama.cpp",
      "versions": [
        "< b5721"
      ]
    }
  ],
  "Reference": [
    "https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-7rxv-5jhh-j6xx",
    "https://github.com/ggml-org/llama.cpp/commit/dd6e6d0b6a4bbe3ebfc931d1eb14db2f2b1d70af"
  ]
}